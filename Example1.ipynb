{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3a307-a161-4d03-81b6-12388f7cbe3d",
   "metadata": {},
   "source": [
    "# Time Series & Historical Query Analysis\n",
    "\n",
    "Welcome to this example where we'll demonstrate how to work with large datasets in kdb+ to analyze time-series data. \n",
    "\n",
    "One of the key features of kdb+ is its ability to handle huge volumes of data with exceptional speed and efficiency. Whether it's reading massive datasets, performing time-based aggregations, or joining data from different sources, kdb+ excels at time-series analysis. By the end of this example, you'll have a clear understanding of how to create, manipulate, store, and analyze data using kdb+/q. Along the way, we'll introduce several key concepts that are fundamental to working with kdb+/q.\n",
    "\n",
    "\n",
    "Here, we'll cover:\n",
    "- Creating a large time-series dataset from scratch\n",
    "- Saving this data to a database on disk\n",
    "- Scaling database to 1 Billion rows\n",
    "- Performing time-based aggregations to analyze trends over time\n",
    "- Using asof joins (aj) to combine time-series data (e.g., matching trades to quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c6c31-ab9a-4349-b0f9-5c541553f284",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "1. For setup instructions and prerequisites, please refer to the [README](README.md).\n",
    "2. Ensure PyKX is properly initialized by running the cell below.<br/>\n",
    "   <b>Note</b>: This is a Python cell that will enable the kernel to execute q code as the default language for all later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKX now running in 'jupyter_qfirst' mode. All cells by default will be run as q code. \n",
      "Include '%%py' at the beginning of each cell to run as python code. \n"
     ]
    }
   ],
   "source": [
    "import pykx as kx\n",
    "kx.util.jupyter_qfirst_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203051bc-25ce-441f-ac35-e078d2499a38",
   "metadata": {},
   "source": [
    "## 2. Create the Time Series Dataset\n",
    "\n",
    "Letâ€™s start by creating a sample dataset to work with. This dataset will simulate trade data over a period of time, with random values for price, size, and symbols. Weâ€™ll generate 20 million rows of trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b63cdf-8139-4b53-810f-3084fdb1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:20000000\n",
    "syms:100?`3\n",
    "day:2025.01.01\n",
    "trade:([] \n",
    "    time:asc (`timestamp$day) + n?24:00:00;              // Start from midnight, spread across 24h\n",
    "    sym:n?syms;                                          // Random stock tickers\n",
    "    price:n?100f;                                        // Random trade prices\n",
    "    size:n?1000                                          // Random trade sizes\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691b30-afb5-4f3b-b9d5-680537770eb9",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening:\n",
    "- `n: 2000000` sets the number of rows we want to generate\n",
    "- We define a new table with table notation `([] col1:<values>; col2:<values>: ...)`\n",
    "- We use `?` to generate random values for 4 columns:\n",
    "    - `time` is populated with timestamps starting from midnight and increasing across a 24-hour period, with a random offset to simulate a spread of trades.\n",
    "    - `sym` is populated with random symbols, selected from a list.\n",
    "    - `price` and trade `size` are randomnly generated\n",
    "\n",
    "This table is now available in memory to investigate and query. Let's take a quick look at the row [`count`](#https://code.kx.com/q/ref/count/), schema details with [`meta`](#https://code.kx.com/q/ref/meta/) and first 10 rows using [`sublist`](#https://code.kx.com/q/ref/sublist/).\n",
    "\n",
    "These simple commands are essential when exploring your data quickly in kdb+/q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfb0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000\n"
     ]
    }
   ],
   "source": [
    "count trade              // get row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748c29a6-ff5d-477f-ac9b-18ff47fc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "time | p   s\n",
      "sym  | s    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade               // get table schema details - datatypes, column names etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d60c8",
   "metadata": {},
   "source": [
    "The following columns are produced when we run `meta`:\n",
    "- c: column name\n",
    "- t: <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">column type</a>\n",
    "- f: <a href=\"https://code.kx.com/q4m3/8_Tables/#85-foreign-keys-and-virtual-columns\" target=\"_blank\">foreign keys</a>\n",
    "- a: <a href=\"https://code.kx.com/q/ref/set-attribute/\" target=\"_blank\">attributes</a> (modifiers applied for performance optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1a6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                          sym price    size\n",
      "-----------------------------------------------\n",
      "2025.01.01D00:00:00.000000000 fln 1.967696 82  \n",
      "2025.01.01D00:00:00.000000000 jpa 59.20225 505 \n",
      "2025.01.01D00:00:00.000000000 lag 4.720372 285 \n",
      "2025.01.01D00:00:00.000000000 lig 10.19818 706 \n",
      "2025.01.01D00:00:00.000000000 had 92.14602 267 \n",
      "2025.01.01D00:00:00.000000000 jfo 5.953175 490 \n",
      "2025.01.01D00:00:00.000000000 hmd 8.237707 167 \n",
      "2025.01.01D00:00:00.000000000 pek 5.712376 457 \n",
      "2025.01.01D00:00:00.000000000 bgj 48.08972 535 \n",
      "2025.01.01D00:00:00.000000000 fna 27.63048 248 \n"
     ]
    }
   ],
   "source": [
    "10 sublist trade         // get first 10 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2a1d-2ca1-4235-a6d9-28779a361fdd",
   "metadata": {},
   "source": [
    "## 3.  Save Data to Disk\n",
    "\n",
    "Once the data is generated, youâ€™ll likely want to save it to disk for persistent storage.\n",
    "\n",
    "Because we want the ability to scale, partitioning by date will be a good approach for this dataset. Without partitioning, queries that span large time periods would require scanning entire datasets, which can be very slow and resource-intensive. By partitioning data, kdb+ can limit the query scope to the relevant partitions, significantly speeding up the process.\n",
    "\n",
    "To partition by date we can use the inbuilt function [`.Q.dpft`](#https://code.kx.com/q/ref/dotq/#dpft-save-table) to save the data to disk - this may take ~20 seconds to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e9616f-87f5-4456-99d2-da888625bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeDir:getenv[`HOME]                   // Get the home directory for edu.kx.com\n",
    "dbDir:homeDir,\"/data\"                   // Define database location as string\n",
    "dbPath:hsym `$dbDir                     // Database location as hsym for file I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c36306-d885-41dd-a292-8b24a9f7a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    ".z.zd:(17;2;6)                          // Set compression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070de1a-2796-47ca-935b-d2fd38d0ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    ".Q.dpft[dbPath;day;`sym;`trade]         // Save data as a partitioned database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914853",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- <a href=\"https://code.kx.com/q/ref/hsym/\" target=\"_blank\">hsym</a>: This function prefixes the directory location with a colon to make it a file handle\n",
    "- <a href=\"https://code.kx.com/q/ref/dotz/#zzd-compressionencryption-defaults\" target=\"_blank\">.z.d</a>: This function sets the compression parameters.\n",
    "- <a href=\"https://code.kx.com/q/ref/dotq/#dpft-save-table\" target=\"_blank\">.Q.dpft[d;p;f;t]</a>: This command saves data to a <b>(d)</b>atabase location, targeting a particular <b>(p)</b>artition and indexes the data on a chosen <b>(f)</b>ield for the specified <b>(t)</b>able.\n",
    "\n",
    "One persisted, the table name is returned. We can test its worked as expected by deleting the `trade` table we have in memory and reloading the database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     // Delete in memory table\n",
    "system\"l \",dbDir                         // Load the partitioned database\n",
    "meta trade                               // Check it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bdda4",
   "metadata": {},
   "source": [
    "kdb+ actually offers a number of different methods to store tables which will allow for efficient storage and querying for different sized datasets: flat, splayed, partitioned and segmented.\n",
    "\n",
    "A general rule of thumb around which format to choose depends on three things:\n",
    "\n",
    "- Will the table continue to grow at a fast rate?\n",
    "- Am I working in a RAM/memory constrained environment?\n",
    "- What level of performance do I want?\n",
    "\n",
    "To learn more about these types and when to choose which <a href=\"https://code.kx.com/q/database/\" target=\"_blank\">see here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb2d99-5516-4bd7-8c37-bd58a6c0cd35",
   "metadata": {},
   "source": [
    "## 4 Scaling Dataset to 1 Billion Rows\n",
    "\n",
    "In this section, we scale our dataset to 1 billion rows by duplicating an existing partition across multiple days. This approach ensures we have sufficient data for performance testing and analytics validation.\n",
    "\n",
    "Before making copies, we check the disk space usage to ensure enough storage is available. The below system command displays the available and used disk space in megabytes (~9.6G), helping us monitor the impact of our operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9eb5f8-2106-4dc1-930f-4ba7f38a3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdm        9.8G  183M  9.6G   2% /home/jovyan\"\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ad551-c666-4120-849a-8940ad34929e",
   "metadata": {},
   "source": [
    "Next, we generate a list of new dates and copy the existing partition (2025.01.01) to these new dates. Here, we create 49 new partitions by copying the original partition for each additional day. This may take ~40 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b06d4b8-a8c5-42be-b9c6-7f3241b017f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55266\n"
     ]
    }
   ],
   "source": [
    "days:day +1 +til 49;                                         // Generate 49 additional days  \n",
    "cmds: \"cp -r ../data/2025.01.01 ../data/\",/:string[days];    // Create shell commands to execute\n",
    "\\t system each cmds                                          // Execute shell commands to copy partitions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e8193-91ac-4eef-9db6-bb15513ca9be",
   "metadata": {},
   "source": [
    "Once the partitions are created, we verify how much disk space was consumed and check the new partitions exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0497fc63-2117-4653-894a-3790b4ae5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdm        9.8G  8.9G  893M  92% /home/jovyan\"\n",
      "\"total 212\"\n",
      "\"drwxrwsr-x 52 jovyan users 4096 Mar  6 13:55 .\"\n",
      "\"drwxrwsr-x  9 root   users 4096 Mar  6 10:20 ..\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:46 2025.01.01\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.02\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.03\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.04\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.05\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.06\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.07\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.08\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.09\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.10\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.11\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.12\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.13\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.14\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.15\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.16\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.17\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.18\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  6 13:54 2025.01.19\"\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\"\n",
    "system\"ls -la ../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e21679-4e18-4982-8a4a-dc164db658fe",
   "metadata": {},
   "source": [
    "Finially since kdb+ manages partitioned data at the filesystem level, we must reload the database to reflect the newly added partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88481f47-c923-410f-ae2f-6b6741686c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "1000000000\n",
      "date      | x       \n",
      "----------| --------\n",
      "2025.01.01| 20000000\n",
      "2025.01.02| 20000000\n",
      "2025.01.03| 20000000\n",
      "2025.01.04| 20000000\n",
      "2025.01.05| 20000000\n",
      "2025.01.06| 20000000\n",
      "2025.01.07| 20000000\n",
      "2025.01.08| 20000000\n",
      "2025.01.09| 20000000\n",
      "2025.01.10| 20000000\n",
      "2025.01.11| 20000000\n",
      "2025.01.12| 20000000\n",
      "2025.01.13| 20000000\n",
      "2025.01.14| 20000000\n",
      "2025.01.15| 20000000\n",
      "2025.01.16| 20000000\n",
      "2025.01.17| 20000000\n",
      "2025.01.18| 20000000\n",
      "2025.01.19| 20000000\n",
      "2025.01.20| 20000000\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                               // Delete in memory table\n",
    "system\"l \",dbDir                                   // Load the partitioned database\n",
    "count trade                                        // 1B Rows \n",
    "select count i by date from trade                  // Select number of records per date within the trade table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ac59",
   "metadata": {},
   "source": [
    "## 5. Time Series Analytics\n",
    "\n",
    "Now that we have some data, let's dive into some basic time-series analytics.\n",
    "\n",
    "### Total Trade Volume Every Hour\n",
    "\n",
    "Let's find a symbol to analyse from the randomly generated list we created earlier and then run our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| size   \n",
      "-----------------| -------\n",
      "2025.01.01 00:00 | 4038856\n",
      "2025.01.01 01:00 | 4214955\n",
      "2025.01.01 02:00 | 4099337\n",
      "2025.01.01 03:00 | 4113799\n",
      "2025.01.01 04:00 | 4133509\n",
      "2025.01.01 05:00 | 4071891\n",
      "2025.01.01 06:00 | 4175473\n",
      "2025.01.01 07:00 | 4130991\n",
      "2025.01.01 08:00 | 4172108\n",
      "2025.01.01 09:00 | 4106918\n",
      "2025.01.01 10:00 | 4086639\n",
      "2025.01.01 11:00 | 4153915\n",
      "2025.01.01 12:00 | 4248459\n",
      "2025.01.01 13:00 | 4113861\n",
      "2025.01.01 14:00 | 4156215\n",
      "2025.01.01 15:00 | 4121299\n",
      "2025.01.01 16:00 | 4154079\n",
      "2025.01.01 17:00 | 4118518\n",
      "2025.01.01 18:00 | 4213332\n",
      "2025.01.01 19:00 | 4199948\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "symbol:first syms\n",
    "select sum size \n",
    "    by date,\n",
    "       60 xbar time.minute \n",
    "    from trade \n",
    "    where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944524",
   "metadata": {},
   "source": [
    "#### qSQL & Temporal Arithmetic\n",
    "Here we are using <a href=\"https://code.kx.com/q/basics/qsql/\" target=\"_blank\">qSQL</a>, the inbuilt table query language in kdb+. If you have used SQL, you will find the syntax of qSQL queries very similar.\n",
    "- Just as in SQL, table results called using `select` and `from` and can be filtered by expressions following a `where`\n",
    "- Multiple filter criteria, separated by ,, are evaluated starting from the left\n",
    "- To group similar values together we can use the `by` clause. This is particularly useful in combination with used with an aggregation like `sum`,`max`,`min` etc.\n",
    "\n",
    "kdb+/q supports several temporal types and arithmetic between them. See here for a summary of <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">datatypes</a>.\n",
    "\n",
    "In this example:\n",
    "- The `time` column in the data has a type of timestamp, which includes both date and time values.\n",
    "- We convert the `time` values to their minute values (including hours and minutes)\n",
    "- We then aggregate further on time by using <a href=\"https://code.kx.com/q/ref/xbar/\" target=\"_blank\">xbar</a> to bucket the minutes into hours (60-unit buckets)\n",
    "\n",
    "### Weighted Average Price and Last Trade Price Every 15 Minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1172a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| lastPx   vwapPx  \n",
      "-----------------| -----------------\n",
      "2025.01.01 00:00 | 9.279603 49.68616\n",
      "2025.01.01 00:15 | 93.50103 49.17632\n",
      "2025.01.01 00:30 | 13.1666  50.20971\n",
      "2025.01.01 00:45 | 37.99301 49.52158\n",
      "2025.01.01 01:00 | 4.45246  49.35297\n",
      "2025.01.01 01:15 | 38.27883 50.20757\n",
      "2025.01.01 01:30 | 36.24516 50.12155\n",
      "2025.01.01 01:45 | 21.79155 50.34161\n",
      "2025.01.01 02:00 | 70.82111 49.8376 \n",
      "2025.01.01 02:15 | 77.94584 49.15689\n",
      "2025.01.01 02:30 | 45.79998 50.07452\n",
      "2025.01.01 02:45 | 89.54561 49.95901\n",
      "2025.01.01 03:00 | 4.223374 50.35866\n",
      "2025.01.01 03:15 | 82.03917 49.60781\n",
      "2025.01.01 03:30 | 18.97577 48.99254\n",
      "2025.01.01 03:45 | 28.4487  51.41991\n",
      "2025.01.01 04:00 | 11.68202 50.00599\n",
      "2025.01.01 04:15 | 19.85271 49.92692\n",
      "2025.01.01 04:30 | 9.235386 50.66791\n",
      "2025.01.01 04:45 | 48.49472 49.65337\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    " by date, 15 xbar time.minute \n",
    " from trade \n",
    " where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc755377",
   "metadata": {},
   "source": [
    "This is similar to the previous analytic, but this time we make use of the built in `wavg` function to find out the weighted average over time intervals. \n",
    "\n",
    "In finance, volume-weighted averages give a more accurate reflection of a stockâ€™s price movement by incorporating trading volume at different price levels. This can be especially useful in understanding whether a price move is supported by strong market participation or is just a result of a few trades.\n",
    "\n",
    "Let's time this anayltic with `\\t` to see how long it takes in milliseconds to crunch through 1 Billion records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d040fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "\\t select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    "   by date, 15 xbar time.minute \n",
    "   from trade \n",
    "   where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea65524",
   "metadata": {},
   "source": [
    "The query processed 1 Billion records in sub second time, efficiently aggregating last price (`lastPx`) and volume-weighted-average price (`vwapPx`) for MSFT trades. The use of `by date, 15 xbar time.minute` optimized the grouping, making the computation fast. This demonstrates the power of kdb+/q for high-speed time-series analytics.\n",
    "\n",
    " ### SQL Comparison\n",
    "\n",
    "A SQL version of this query above would look something like:\n",
    "\n",
    "```\n",
    "\n",
    "SELECT \n",
    "    (array_agg(price ORDER BY time DESC))[1] AS lastPx,\n",
    "    SUM(price * size) / NULLIF(SUM(size), 0) AS vwapPx,\n",
    "    DATE_TRUNC('day', time),                                            \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE) \n",
    "FROM \n",
    "    trade\n",
    "WHERE \n",
    "    sym = 'MSFT'\n",
    "GROUP BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE)\n",
    "ORDER BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE);\n",
    "\n",
    "```\n",
    "\n",
    "SQL is more complex due to several factors:\n",
    "- **Time-series Calculations**: The SQL version involves the creation of custom logic for common time-series calculations such as volume-weighted-averages. In the q-sql version, these functionalities are implicit, and the syntax is more concise when working with vectors. The SQL equivalent requires custom definitions and is often more verbose leaving room for error.\n",
    "- **Grouping and Aggregation**: In the q-sql version, grouping by date and a 15 minute window is done with a single, simple syntax, which is an efficient and intuitive way to express time bucketing. In SQL, similar behavior requires explicitly defining how time intervals are handled and aggregating the results using GROUP BY with custom time expressions which are often repeated throughout the query.\n",
    "- **Temporal Formatting**: SQL queries often require repetitive conversion for handling timestamp formats, which is more cumbersome compared to q-sql, where time-based operations like xbar (interval-based bucketing) can be done directly in a streamlined manner. Temporal primitives also make it extremely easy to convert a nanosecond timestamp to it's equivalent minute using dot notation e.g. time.minute\n",
    "- **Data Transformation**: The q language is optimized for high-performance, in-memory, columnar data transformations, which allows for more compact expressions on vectors of data. SQL, on the other hand, is typically too general purpose for even simple transformations on time-series data. This is down to how kdb+/q is designed, where operations execute on ordered lists, whereas SQL (based on set theory) treats data as records instead of columns e.g. selecting the (last) value in a series, or understanding prior states (deltas) for series movements would require re-ordering the column data\n",
    "- **Performance Considerations**: q-sql is designed for high-performance analytics on large datasets, and many operations that would require complex SQL expressions can be done efficiently with q-sql syntax. In SQL, complex operations requires workarounds such as additional processing with temporary tables, sub-expressions, re-indexing, changing data models, or heavily leveraging partitions and window functions.\n",
    "\n",
    "Thus, while the core logic of the query is similar in both languages, the SQL version requires much more overhead in terms of complexity and verbosity. This inefficiency will also become more pronounced with large datasets, leading to challenges with query performance.\n",
    "\n",
    "While these are just basic analytics, they highlight kdb+/qâ€™s ability to storage and analyse large-scale time-series datasets quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b806e",
   "metadata": {},
   "source": [
    "## 6. Asof Join â€“ Matching Trades with Quotes\n",
    "\n",
    "One of the most powerful features in kdb+/q is the asof join (`aj`), which is designed to match records from two tables based on the most recent timestamp. Unlike a standard SQL join, where records must match exactly on a key, an asof join finds the most recent match.\n",
    "\n",
    "Why Use Asof Joins?\n",
    "In time-series data, we often deal with information arriving at different intervals. For example:\n",
    "- Trade and Quote Data: A trade occurs at a given time, and we want to match it with the latest available quote.\n",
    "- Sensor Data: A sensor records temperature every second, while another logs environmental data every 10 secondsâ€”matching the closest reading is crucial.\n",
    "\n",
    "> **ðŸ“Œ** kdb+/q optimizes asof joins to handle large datasets efficiently, making it a key tool in real-time analytics and historical data analysis.\n",
    "\n",
    "#### Generate synthetic quote data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:2000000\n",
    "quote:([] \n",
    "    time:asc (`timestamp$day) + n?86400000000000;  // Random timestamps\n",
    "    sym:n?syms;                                    // Random stock tickers\n",
    "    bid:n?100f;                                    // Random bid prices\n",
    "    ask:n?100f                                     // Random ask prices\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0b9f",
   "metadata": {},
   "source": [
    "As we're keeping this table in memory we need to perform one extra step before joining, we apply the parted (p#) attribute to the sym column of the quote table. Our trade table on disk already has the parted attribute on the sym column, we see this in the column `a` when we run `meta trade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b4c9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781da43",
   "metadata": {},
   "source": [
    "This is crucial for optimizing asof joins, as it ensures faster lookups when performing symbol-based joins. Before applying parted to quote, we first sort the table by sym using [`xasc`](#https://code.kx.com/q/ref/asc/), as the parted attribute requires the column to be sorted for it to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "458e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote:`sym xasc quote                  / Sorting sym in ascending order\n",
    "quote:update `p#sym from quote         / Apply the parted attruibute on sym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fd87b",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `xasc` Sorts the quote table by sym in ascending order\n",
    "- `#`  Applies the parted attribute to sym, optimizing symbol-based lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a1a4",
   "metadata": {},
   "source": [
    "#### Peform Asof Join\n",
    "\n",
    "We now match each trade with the most recent available quote for todays date using [`aj`](#https://code.kx.com/q/ref/aj/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0bb0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       sym time                          price    size bid      ask     \n",
      "----------------------------------------------------------------------------\n",
      "2025.01.01 abj 2025.01.01D00:00:00.000000000 49.9322  112                   \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 77.34705 780                   \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 9.322147 266                   \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 3.533422 634                   \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 52.95851 729                   \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 15.51982 86                    \n",
      "2025.01.01 abj 2025.01.01D00:00:01.000000000 56.5147  393                   \n",
      "2025.01.01 abj 2025.01.01D00:00:02.000000000 37.03945 164                   \n",
      "2025.01.01 abj 2025.01.01D00:00:03.000000000 96.78387 765                   \n",
      "2025.01.01 abj 2025.01.01D00:00:03.000000000 86.67517 899                   \n",
      "2025.01.01 abj 2025.01.01D00:00:03.000000000 56.11418 396                   \n",
      "2025.01.01 abj 2025.01.01D00:00:03.000000000 57.76882 811                   \n",
      "2025.01.01 abj 2025.01.01D00:00:04.000000000 31.53938 763  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:04.000000000 48.00507 358  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:04.000000000 82.24584 265  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:04.000000000 88.27194 368  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:05.000000000 6.678748 257  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:05.000000000 61.30211 833  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:06.000000000 22.72437 426  49.44834 88.69571\n",
      "2025.01.01 abj 2025.01.01D00:00:06.000000000 56.47551 54   49.44834 88.69571\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "tradequote:aj[`sym`time; select from trade where date=day; quote]\n",
    "tradequote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd376e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `aj` performs an asof join on the `sym` and `time` columns\n",
    "- Each trade record gets matched with the latest available quote at or before the tradeâ€™s timestamp.\n",
    "- We can see this means the first few `bid` and `ask` values are empty because there was no quote data prior to those trades.\n",
    "\n",
    "This approach ensures that for every trade, we have the best available quote information, allowing traders to analyze trade execution relative to the prevailing bid/ask spread at the time.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try [Example2](Example2.html) on Real-Time Ingestion & Streaming Analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
