{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3a307-a161-4d03-81b6-12388f7cbe3d",
   "metadata": {},
   "source": [
    "# Time Series & Historical Query Analysis\n",
    "\n",
    "Welcome to this example where we'll demonstrate how to work with large datasets in kdb+ to analyze time-series data. \n",
    "\n",
    "One of the key features of kdb+ is its ability to handle huge volumes of data with exceptional speed and efficiency. Whether it's reading massive datasets, performing time-based aggregations, or joining data from different sources, kdb+ excels at time-series analysis. By the end of this example, you'll have a clear understanding of how to create, manipulate, store, and analyze data using kdb+/q. Along the way, we'll introduce several key concepts that are fundamental to working with kdb+/q.\n",
    "\n",
    "\n",
    "Here, we'll cover:\n",
    "- Creating a large time-series dataset from scratch\n",
    "- Saving this data to a database on disk\n",
    "- Streamline ingestion and save down using functions \n",
    "- Performing time-based aggregations to analyze trends over time\n",
    "- Using asof joins (aj) to combine time-series data (e.g., matching trades to quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c6c31-ab9a-4349-b0f9-5c541553f284",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "1. For setup instructions and prerequisites, please refer to the [README](README.md).\n",
    "2. Ensure PyKX is properly initialized by running the cell below.<br/>\n",
    "   <b>Note</b>: This is a Python cell that will enable the kernel to execute q code as the default language for all later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKX now running in 'jupyter_qfirst' mode. All cells by default will be run as q code. \n",
      "Include '%%py' at the beginning of each cell to run as python code. \n"
     ]
    }
   ],
   "source": [
    "import pykx as kx\n",
    "kx.util.jupyter_qfirst_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18b9b41-fff3-4421-8cdc-6a49782e21ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used| 2076560\n",
      "heap| 67108864\n",
      "peak| 67108864\n",
      "wmax| 0\n",
      "mmap| 0\n",
      "mphy| 67436519424\n",
      "syms| 3712\n",
      "symw| 188782\n"
     ]
    }
   ],
   "source": [
    ".Q.w[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0e4e9a-b892-4284-bf83-234f1b3e7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdh        9.8G  792K  9.8G   1% /home/jovyan\"\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203051bc-25ce-441f-ac35-e078d2499a38",
   "metadata": {},
   "source": [
    "## 2. Create the Time Series Dataset\n",
    "\n",
    "Let’s start by creating a sample dataset to work with. This dataset will simulate trade data over a period of time, with random values for price, size, and symbols. We’ll generate 5 million rows of trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b63cdf-8139-4b53-810f-3084fdb1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:10000000\n",
    "day:2025.01.01\n",
    "trade:([] \n",
    "    time:asc (`timestamp$day) + n?24:00:00;              / Start from midnight, spread across 24h\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                          / Random stock tickers\n",
    "    price:n?100f;                                        / Random trade prices\n",
    "    size:n?1000                                          / Random trade sizes\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691b30-afb5-4f3b-b9d5-680537770eb9",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening:\n",
    "- `n: 5000000` sets the number of rows we want to generate\n",
    "- We define a new table with table notation `([] col1:<values>; col2:<values>: ...)`\n",
    "- We use `?` to generate random values for 4 columns:\n",
    "    - `time` is populated with timestamps starting from midnight and increasing across a 24-hour period, with a random offset to simulate a spread of trades.\n",
    "    - `sym` is populated with random symbols like AAPL, MSFT, etc., selected from a list.\n",
    "    - `price` and trade `size` are randomnly generated\n",
    "\n",
    "This table is now available in memory to investigate and query. Let's take a quick look at the row [`count`](#https://code.kx.com/q/ref/count/), schema details with [`meta`](#https://code.kx.com/q/ref/meta/) and first 10 rows using [`sublist`](#https://code.kx.com/q/ref/sublist/).\n",
    "\n",
    "These simple commands are essential when exploring your data quickly in kdb+/q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfb0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n"
     ]
    }
   ],
   "source": [
    "count trade              / get row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748c29a6-ff5d-477f-ac9b-18ff47fc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "time | p   s\n",
      "sym  | s    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade               / get table schema details - datatypes, column names etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d60c8",
   "metadata": {},
   "source": [
    "The following columns are produced when we run `meta`:\n",
    "- c: column name\n",
    "- t: <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">column type</a>\n",
    "- f: <a href=\"https://code.kx.com/q4m3/8_Tables/#85-foreign-keys-and-virtual-columns\" target=\"_blank\">foreign keys</a>\n",
    "- a: <a href=\"https://code.kx.com/q/ref/set-attribute/\" target=\"_blank\">attributes</a> (modifiers applied for performance optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1a6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                          sym  price    size\n",
      "------------------------------------------------\n",
      "2025.01.01D00:00:00.000000000 AMZN 4.037237 655 \n",
      "2025.01.01D00:00:00.000000000 AMZN 13.76677 649 \n",
      "2025.01.01D00:00:00.000000000 MSFT 35.25838 934 \n",
      "2025.01.01D00:00:00.000000000 AMZN 22.24729 348 \n",
      "2025.01.01D00:00:00.000000000 MSFT 24.44719 593 \n",
      "2025.01.01D00:00:00.000000000 GOOG 60.64019 20  \n",
      "2025.01.01D00:00:00.000000000 AAPL 12.23558 825 \n",
      "2025.01.01D00:00:00.000000000 MSFT 31.32998 628 \n",
      "2025.01.01D00:00:00.000000000 AMZN 21.48323 500 \n",
      "2025.01.01D00:00:00.000000000 AMZN 17.12322 402 \n"
     ]
    }
   ],
   "source": [
    "10 sublist trade         / get first 10 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2a1d-2ca1-4235-a6d9-28779a361fdd",
   "metadata": {},
   "source": [
    "## 3.  Save Data to Disk\n",
    "\n",
    "Once the data is generated, you’ll likely want to save it to disk for persistent storage.\n",
    "\n",
    "Because we want the ability to scale, partitioning by date will be a good approach for this dataset. Without partitioning, queries that span large time periods would require scanning entire datasets, which can be very slow and resource-intensive. By partitioning data, kdb+ can limit the query scope to the relevant partitions, significantly speeding up the process.\n",
    "\n",
    "To partition by date we can use the inbuilt function [`.Q.dpft`](#https://code.kx.com/q/ref/dotq/#dpft-save-table).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e9616f-87f5-4456-99d2-da888625bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeDir:getenv[`HOME]                    / Get the home directory for edu.kx.com\n",
    "dbDir:homeDir,\"/data\"                    / Define database location as string\n",
    "dbPath:hsym `$dbDir                      / Database location as hsym for file I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c36306-d885-41dd-a292-8b24a9f7a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    ".z.zd:(17;2;6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a070de1a-2796-47ca-935b-d2fd38d0ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n"
     ]
    }
   ],
   "source": [
    ".Q.dpft[dbPath;day;`sym;`trade]          / Save data as a partitioned database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914853",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- <a href=\"https://code.kx.com/q/ref/hsym/\" target=\"_blank\">hsym</a>: This function prefixes the directory location with a colon to make it a file handle\n",
    "- <a href=\"https://code.kx.com/q/ref/dotq/#dpft-save-table\" target=\"_blank\">.Q.dpft[d;p;f;t]</a>: This command saves data to a <b>(d)</b>atabase location, targeting a particular <b>(p)</b>artition and indexes the data on a chosen <b>(f)</b>ield for the specified <b>(t)</b>able.\n",
    "\n",
    "One persisted, the table name is returned. We can test its worked as expected by deleting the `trade` table we have in memory and reloading the database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     / Delete in memory table\n",
    "system\"l \",dbDir                         / Load the partitioned database\n",
    "meta trade                               / Check it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bdda4",
   "metadata": {},
   "source": [
    "kdb+ actually offers a number of different methods to store tables which will allow for efficient storage and querying for different sized datasets: flat, splayed, partitioned and segmented.\n",
    "\n",
    "A general rule of thumb around which format to choose depends on three things:\n",
    "\n",
    "- Will the table continue to grow at a fast rate?\n",
    "- Am I working in a RAM/memory constrained environment?\n",
    "- What level of performance do I want?\n",
    "\n",
    "To learn more about these types and when to choose which <a href=\"https://code.kx.com/q/database/\" target=\"_blank\">see here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0265c3d",
   "metadata": {},
   "source": [
    "## 4. Scaling Data Ingestion with Functions\n",
    "\n",
    "If you want to scale the ingestion of data to many days, it’s helpful to create a reusable function. Let’s create a function `createTrade` that generates trade data for specific dates and saves it to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57821f5a-f8f2-400d-a70c-a68b72aebd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used| 2079888\n",
      "heap| 1543503872\n",
      "peak| 1543503872\n",
      "wmax| 0\n",
      "mmap| 0\n",
      "mphy| 67436519424\n",
      "syms| 3753\n",
      "symw| 190598\n"
     ]
    }
   ],
   "source": [
    ".Q.w[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "913c0a8f-f2f4-4cc2-bcc5-43fdf87d2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\c 100 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46ebc4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "QError",
     "evalue": "noupdate: `. `trade",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQError\u001b[0m                                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreateTrade:\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m[date]                                            / Start of function definition and input parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trade::([]                                                 / Start of table definition\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m              time:asc (`timestamp$date) + n?24:00:00;         / Start from midnight, spread across 24h\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m              sym:n?`AAPL`MSFT`GOOG`AMZN;                      / Random stock symbols\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m              price:n?100f;                                    / Random trade prices\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m              size:n?1000                                      / Random trade sizes\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        );                                                     / End of table definition\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .Q.dpft[dbPath;date;`sym;`trade]                           / Save data as a partitioned database\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m }                                                             / End of function definition\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdays:day + 1 + til 30                                        / Generate a list of 5 dates\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mt createTrade peach days                                          / Execute the function for each date in the list\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykx/nbextension.py:181\u001b[0m, in \u001b[0;36mq\u001b[0;34m(instructions, code)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCell contents not saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m due to error during execution/saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(_q), kx\u001b[38;5;241m.\u001b[39mQConnection):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pykx/nbextension.py:169\u001b[0m, in \u001b[0;36mq\u001b[0;34m(instructions, code)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug \u001b[38;5;129;01mor\u001b[39;00m kx\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpykx_qdebug:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mprint\u001b[39m(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kx\u001b[38;5;241m.\u001b[39mQError(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpy()\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     display(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m displayRet \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mQError\u001b[0m: noupdate: `. `trade"
     ]
    }
   ],
   "source": [
    "createTrade:{[date]                                            / Start of function definition and input parameters\n",
    "    trade::([]                                                 / Start of table definition\n",
    "              time:asc (`timestamp$date) + n?24:00:00;         / Start from midnight, spread across 24h\n",
    "              sym:n?`AAPL`MSFT`GOOG`AMZN;                      / Random stock symbols\n",
    "              price:n?100f;                                    / Random trade prices\n",
    "              size:n?1000                                      / Random trade sizes\n",
    "        );                                                     / End of table definition\n",
    "    .Q.dpft[dbPath;date;`sym;`trade]                           / Save data as a partitioned database\n",
    " }                                                             / End of function definition\n",
    "\n",
    "days:day + 1 + til 30                                        / Generate a list of 5 dates\n",
    "\\t createTrade peach days                                          / Execute the function for each date in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ed997d-5465-4e1b-aa6a-92a839a476e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k){[d;p;f;t;s]if[` in f,c:!+r:`. . `\\:t;'`domain];if[~f in c;'f];i:<t f;r:+enxs[$;d;r;s];{[d;t;i;u;x]@[d;x;:;u t[x]i]}[d:par[d;p;t];r;i;]'[(::;`p#)f=c;c];@[d;`.d;:;f,c@&~f=c];t}[;;;;`sym]\n"
     ]
    }
   ],
   "source": [
    ".Q.dpft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96240940-f926-41d1-9d03-5fdc78336d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used| 538954784\n",
      "heap| 1811939328\n",
      "peak| 1811939328\n",
      "wmax| 0\n",
      "mmap| 0\n",
      "mphy| 67436519424\n",
      "syms| 4175\n",
      "symw| 216253\n"
     ]
    }
   ],
   "source": [
    ".Q.w[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3772bbd-3cc7-4a47-b5c2-6681bc04ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdh        9.8G  2.4G  7.5G  24% /home/jovyan\"\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba268e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- The function `createTrade` generates trade data for a given date, and then saves it to disk.\n",
    "- We generate data for multiple days (2025.02.01 to 2025.02.05), using the [`til`](#https://code.kx.com/q/ref/til/) operator as a quick handy way to generate a list of dates.\n",
    "- The we loop over the dates using [`each`](#https://code.kx.com/q/wp/iterators/#map-iterators)\n",
    "\n",
    "> **📌 Iterators** like each are the primary means of iteration in q, and in almost all cases the most efficient way to iterate. Loops are rare in q programs and are almost always candidates for optimization.\n",
    "\n",
    "After running this function, the data will be partitioned and stored for each specific day. Again, lets delete our in memory `trade` table and reload our database to pick up these new additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d876cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "date      | x       \n",
      "----------| --------\n",
      "2025.01.01| 10000000\n",
      "2025.01.02| 10000000\n",
      "2025.01.03| 10000000\n",
      "2025.01.04| 10000000\n",
      "2025.01.05| 10000000\n",
      "2025.01.06| 10000000\n",
      "2025.01.07| 10000000\n",
      "2025.01.08| 10000000\n",
      "2025.01.09| 10000000\n",
      "2025.01.10| 10000000\n",
      "2025.01.11| 10000000\n",
      "2025.01.12| 10000000\n",
      "2025.01.13| 10000000\n",
      "2025.01.14| 10000000\n",
      "2025.01.15| 10000000\n",
      "2025.01.16| 10000000\n",
      "2025.01.17| 10000000\n",
      "2025.01.18| 10000000\n",
      "2025.01.19| 10000000\n",
      "2025.01.20| 10000000\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                               / Delete in memory table\n",
    "system\"l \",dbDir                                   / Load the partitioned database\n",
    "select count i by date from trade                  / Select number of records per date within the trade table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ac59",
   "metadata": {},
   "source": [
    "## 5. Time Series Analytics\n",
    "\n",
    "Now that we have some data, let's dive into some basic time-series analytics.\n",
    "\n",
    "### Total Trade Volume Every Hour for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd0f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| size    \n",
      "-----------------| --------\n",
      "2025.01.01 00:00 | 52228952\n",
      "2025.01.01 01:00 | 51872751\n",
      "2025.01.01 02:00 | 52064693\n",
      "2025.01.01 03:00 | 52071918\n",
      "2025.01.01 04:00 | 51904116\n",
      "2025.01.01 05:00 | 51830218\n",
      "2025.01.01 06:00 | 52034075\n",
      "2025.01.01 07:00 | 52037456\n",
      "2025.01.01 08:00 | 52182458\n",
      "2025.01.01 09:00 | 52070061\n",
      "2025.01.01 10:00 | 52082665\n",
      "2025.01.01 11:00 | 51789446\n",
      "2025.01.01 12:00 | 52009891\n",
      "2025.01.01 13:00 | 52153188\n",
      "2025.01.01 14:00 | 52155671\n",
      "2025.01.01 15:00 | 51896148\n",
      "2025.01.01 16:00 | 52020846\n",
      "2025.01.01 17:00 | 51932273\n",
      "2025.01.01 18:00 | 51828075\n",
      "2025.01.01 19:00 | 52103248\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select sum size \n",
    "    by date,\n",
    "       60 xbar time.minute \n",
    "    from trade \n",
    "    where sym=`AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944524",
   "metadata": {},
   "source": [
    "#### qSQL & Temporal Arithmetic\n",
    "Here we are using <a href=\"https://code.kx.com/q/basics/qsql/\" target=\"_blank\">qSQL</a>, the inbuilt table query language in kdb+. If you have used SQL, you will find the syntax of qSQL queries very similar.\n",
    "- Just as in SQL, table results called using `select` and `from` and can be filtered by expressions following a `where`\n",
    "- Multiple filter criteria, separated by ,, are evaluated starting from the left\n",
    "- To group similar values together we can use the `by` clause. This is particularly useful in combination with used with an aggregation like `sum`,`max`,`min` etc.\n",
    "\n",
    "kdb+/q supports several temporal types and arithmetic between them. See here for a summary of <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">datatypes</a>.\n",
    "\n",
    "In this example:\n",
    "- The `time` column in the data has a type of timestamp, which includes both date and time values.\n",
    "- We convert the `time` values to their minute values (including hours and minutes)\n",
    "- We then aggregate further on time by using <a href=\"https://code.kx.com/q/ref/xbar/\" target=\"_blank\">xbar</a> to bucket the minutes into hours (60-unit buckets)\n",
    "\n",
    "### Weighted Average Price and Last Trade Price Every 15 Minutes for MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1172a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| lastPx   vwapPx  \n",
      "-----------------| -----------------\n",
      "2025.01.01 00:00 | 62.99824 49.80043\n",
      "2025.01.01 00:15 | 7.181633 49.96954\n",
      "2025.01.01 00:30 | 53.54667 50.10013\n",
      "2025.01.01 00:45 | 4.95421  49.88338\n",
      "2025.01.01 01:00 | 84.04677 50.34475\n",
      "2025.01.01 01:15 | 34.85096 49.42832\n",
      "2025.01.01 01:30 | 6.610401 50.0637 \n",
      "2025.01.01 01:45 | 90.43545 50.0949 \n",
      "2025.01.01 02:00 | 5.894815 49.91792\n",
      "2025.01.01 02:15 | 65.51916 50.01561\n",
      "2025.01.01 02:30 | 17.56833 49.91298\n",
      "2025.01.01 02:45 | 43.00724 49.83422\n",
      "2025.01.01 03:00 | 25.91682 50.45375\n",
      "2025.01.01 03:15 | 69.40958 49.89365\n",
      "2025.01.01 03:30 | 32.29328 50.21242\n",
      "2025.01.01 03:45 | 34.90826 50.23979\n",
      "2025.01.01 04:00 | 63.70649 49.54437\n",
      "2025.01.01 04:15 | 51.32217 50.06754\n",
      "2025.01.01 04:30 | 41.98754 50.14077\n",
      "2025.01.01 04:45 | 54.61311 49.98737\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    " by date, 15 xbar time.minute \n",
    " from trade \n",
    " where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc755377",
   "metadata": {},
   "source": [
    "This is similar to the previous analytic, but this time we make use of the built in `wavg` function to find out the weighted average over time intervals. \n",
    "\n",
    "In finance, volume-weighted averages give a more accurate reflection of a stock’s price movement by incorporating trading volume at different price levels. This can be especially useful in understanding whether a price move is supported by strong market participation or is just a result of a few trades.\n",
    "\n",
    "Let's time this anayltic with `\\t` to see how long it takes in milliseconds to crunch through 30 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d040fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n"
     ]
    }
   ],
   "source": [
    "\\t select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    "   by date, 15 xbar time.minute \n",
    "   from trade \n",
    "   where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea65524",
   "metadata": {},
   "source": [
    "The query processed 30+ million records in 147 ms, efficiently aggregating last price (`lastPx`) and volume-weighted-average price (`vwapPx`) for MSFT trades. The use of `by date, 15 xbar time.minute` optimized the grouping, making the computation fast. This demonstrates the power of kdb+/q for high-speed time-series analytics.\n",
    "\n",
    " ### SQL Comparison\n",
    "\n",
    "A SQL version of this query above would look something like:\n",
    "\n",
    "```\n",
    "\n",
    "SELECT \n",
    "    (array_agg(price ORDER BY time DESC))[1] AS lastPx,\n",
    "    SUM(price * size) / NULLIF(SUM(size), 0) AS vwapPx,\n",
    "    DATE_TRUNC('day', time),                                            \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE) \n",
    "FROM \n",
    "    trade\n",
    "WHERE \n",
    "    sym = 'MSFT'\n",
    "GROUP BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE)\n",
    "ORDER BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE);\n",
    "\n",
    "```\n",
    "\n",
    "SQL is more complex due to several factors:\n",
    "- **Time-series Calculations**: The SQL version involves the creation of custom logic for common time-series calculations such as volume-weighted-averages. In the q-sql version, these functionalities are implicit, and the syntax is more concise when working with vectors. The SQL equivalent requires custom definitions and is often more verbose leaving room for error.\n",
    "- **Grouping and Aggregation**: In the q-sql version, grouping by date and a 15 minute window is done with a single, simple syntax, which is an efficient and intuitive way to express time bucketing. In SQL, similar behavior requires explicitly defining how time intervals are handled and aggregating the results using GROUP BY with custom time expressions which are often repeated throughout the query.\n",
    "- **Temporal Formatting**: SQL queries often require repetitive conversion for handling timestamp formats, which is more cumbersome compared to q-sql, where time-based operations like xbar (interval-based bucketing) can be done directly in a streamlined manner. Temporal primitives also make it extremely easy to convert a nanosecond timestamp to it's equivalent minute using dot notation e.g. time.minute\n",
    "- **Data Transformation**: The q language is optimized for high-performance, in-memory, columnar data transformations, which allows for more compact expressions on vectors of data. SQL, on the other hand, is typically too general purpose for even simple transformations on time-series data. This is down to how kdb+/q is designed, where operations execute on ordered lists, whereas SQL (based on set theory) treats data as records instead of columns e.g. selecting the (last) value in a series, or understanding prior states (deltas) for series movements would require re-ordering the column data\n",
    "- **Performance Considerations**: q-sql is designed for high-performance analytics on large datasets, and many operations that would require complex SQL expressions can be done efficiently with q-sql syntax. In SQL, complex operations requires workarounds such as additional processing with temporary tables, sub-expressions, re-indexing, changing data models, or heavily leveraging partitions and window functions.\n",
    "\n",
    "Thus, while the core logic of the query is similar in both languages, the SQL version requires much more overhead in terms of complexity and verbosity. This inefficiency will also become more pronounced with large datasets, leading to challenges with query performance.\n",
    "\n",
    "While these are just basic analytics, they highlight kdb+/q’s ability to storage and analyse large-scale time-series datasets quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b806e",
   "metadata": {},
   "source": [
    "## 6. Asof Join – Matching Trades with Quotes\n",
    "\n",
    "One of the most powerful features in kdb+/q is the asof join (`aj`), which is designed to match records from two tables based on the most recent timestamp. Unlike a standard SQL join, where records must match exactly on a key, an asof join finds the most recent match.\n",
    "\n",
    "Why Use Asof Joins?\n",
    "In time-series data, we often deal with information arriving at different intervals. For example:\n",
    "- Trade and Quote Data: A trade occurs at a given time, and we want to match it with the latest available quote.\n",
    "- Sensor Data: A sensor records temperature every second, while another logs environmental data every 10 seconds—matching the closest reading is crucial.\n",
    "\n",
    "> **📌** kdb+/q optimizes asof joins to handle large datasets efficiently, making it a key tool in real-time analytics and historical data analysis.\n",
    "\n",
    "#### Generate synthetic quote data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:2000000\n",
    "today:last days\n",
    "quote:([] \n",
    "    time:asc (`timestamp$today) + n?86400000000000;  / Random timestamps\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                      / Random stock tickers\n",
    "    bid:n?100f;                                      / Random bid prices\n",
    "    ask:n?100f                                       / Random ask prices\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0b9f",
   "metadata": {},
   "source": [
    "As we're keeping this table in memory we need to perform one extra step before joining, we apply the parted (p#) attribute to the sym column of the quote table. Our trade table on disk already has the parted attribute on the sym column, we see this in the column `a` when we run `meta trade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b4c9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781da43",
   "metadata": {},
   "source": [
    "This is crucial for optimizing asof joins, as it ensures faster lookups when performing symbol-based joins. Before applying parted to quote, we first sort the table by sym using [`xasc`](#https://code.kx.com/q/ref/asc/), as the parted attribute requires the column to be sorted for it to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "458e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote:`sym xasc quote                  / Sorting sym in ascending order\n",
    "quote:update `p#sym from quote         / Apply the parted attruibute on sym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fd87b",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `xasc` Sorts the quote table by sym in ascending order\n",
    "- `#`  Applies the parted attribute to sym, optimizing symbol-based lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a1a4",
   "metadata": {},
   "source": [
    "#### Peform Asof Join\n",
    "\n",
    "We now match each trade with the most recent available quote for todays date using [`aj`](#https://code.kx.com/q/ref/aj/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0bb0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       sym  time                          price    size bid ask\n",
      "-------------------------------------------------------------------\n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 61.19055 427         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 10.30378 701         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 51.7924  908         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 83.30336 700         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 22.94161 326         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 49.01219 12          \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 9.120412 393         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 91.51694 500         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 26.40567 197         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 26.96222 564         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 96.5675  345         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 38.72911 817         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 97.543   246         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 60.15426 69          \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 58.70972 633         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 32.60809 483         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 31.35631 410         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 85.6479  479         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 66.61455 967         \n",
      "2025.01.31 AAPL 2025.01.31D00:00:00.000000000 7.027179 421         \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "tradequote:aj[`sym`time; select from trade where date=today; quote]\n",
    "tradequote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd376e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `aj` performs an asof join on the `sym` and `time` columns\n",
    "- Each trade record gets matched with the latest available quote at or before the trade’s timestamp.\n",
    "- We can see this means the first few `bid` and `ask` values are empty because there was no quote data prior to those trades.\n",
    "\n",
    "This approach ensures that for every trade, we have the best available quote information, allowing traders to analyze trade execution relative to the prevailing bid/ask spread at the time.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try [Example2](Example2.html) on Real-Time Ingestion & Streaming Analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edcdb815-62ff-4f7c-9c3a-937e6f92cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used| 404740272\n",
      "heap| 1946157056\n",
      "peak| 1946157056\n",
      "wmax| 0\n",
      "mmap| 320004328\n",
      "mphy| 67436519424\n",
      "syms| 4242\n",
      "symw| 218937\n"
     ]
    }
   ],
   "source": [
    ".Q.w[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09df3e12-b853-4450-a5c7-00b87a214705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdh        9.8G  9.3G  502M  95% /home/jovyan\"\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6fca44-a5e1-4d3a-9ac4-cc363a57cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75\n"
     ]
    }
   ],
   "source": [
    "// peak diff in GB \n",
    "((.Q.w[]`peak)-67108864)%1073741824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b441150d-9470-44e9-9c44-d9f54d4fbe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000000\n"
     ]
    }
   ],
   "source": [
    "count trade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
