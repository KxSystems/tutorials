{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3a307-a161-4d03-81b6-12388f7cbe3d",
   "metadata": {},
   "source": [
    "# Time Series & Historical Query Analysis\n",
    "\n",
    "Welcome to this example where we'll demonstrate how to work with large datasets in kdb+ to analyze time-series data. \n",
    "\n",
    "One of the key features of kdb+ is its ability to handle huge volumes of data with exceptional speed and efficiency. Whether it's reading massive datasets, performing time-based aggregations, or joining data from different sources, kdb+ excels at time-series analysis. By the end of this example, you'll have a clear understanding of how to create, manipulate, store, and analyze data using kdb+/q. Along the way, we'll introduce several key concepts that are fundamental to working with kdb+/q.\n",
    "\n",
    "\n",
    "Here, we'll cover:\n",
    "- Creating a large time-series dataset from scratch\n",
    "- Saving this data to a database on disk\n",
    "- Streamline ingestion and save down using functions \n",
    "- Performing time-based aggregations to analyze trends over time\n",
    "- Using asof joins (aj) to combine time-series data (e.g., matching trades to quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c6c31-ab9a-4349-b0f9-5c541553f284",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "1. For setup instructions and prerequisites, please refer to the [README](README.md).\n",
    "2. Ensure PyKX is properly initialized by running the cell below.<br/>\n",
    "   <b>Note</b>: This is a Python cell that will enable the kernel to execute q code as the default language for all later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKX now running in 'jupyter_qfirst' mode. All cells by default will be run as q code. \n",
      "Include '%%py' at the beginning of each cell to run as python code. \n"
     ]
    }
   ],
   "source": [
    "import pykx as kx\n",
    "kx.util.jupyter_qfirst_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203051bc-25ce-441f-ac35-e078d2499a38",
   "metadata": {},
   "source": [
    "## 2. Create the Time Series Dataset\n",
    "\n",
    "Letâ€™s start by creating a sample dataset to work with. This dataset will simulate trade data over a period of time, with random values for price, size, and symbols. Weâ€™ll generate 5 million rows of trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b63cdf-8139-4b53-810f-3084fdb1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:5000000\n",
    "day:2025.01.02\n",
    "trade:([] \n",
    "    time:asc (`timestamp$day) + n?24:00:00;              / Start from midnight, spread across 24h\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                          / Random stock tickers\n",
    "    price:n?100f;                                        / Random trade prices\n",
    "    size:n?1000                                          / Random trade sizes\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691b30-afb5-4f3b-b9d5-680537770eb9",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening:\n",
    "- `n: 5000000` sets the number of rows we want to generate\n",
    "- We define a new table with table notation `([] col1:<values>; col2:<values>: ...)`\n",
    "- We use `?` to generate random values for 4 columns:\n",
    "    - `time` is populated with timestamps starting from midnight and increasing across a 24-hour period, with a random offset to simulate a spread of trades.\n",
    "    - `sym` is populated with random symbols like AAPL, MSFT, etc., selected from a list.\n",
    "    - `price` and trade `size` are randomnly generated\n",
    "\n",
    "This table is now available in memory to investigate and query. Let's take a quick look at the row [`count`](#https://code.kx.com/q/ref/count/), schema details with [`meta`](#https://code.kx.com/q/ref/meta/) and first 10 rows using [`sublist`](#https://code.kx.com/q/ref/sublist/).\n",
    "\n",
    "These simple commands are essential when exploring your data quickly in kdb+/q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfb0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n"
     ]
    }
   ],
   "source": [
    "count trade              / get row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748c29a6-ff5d-477f-ac9b-18ff47fc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "time | p   s\n",
      "sym  | s    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade               / get table schema details - datatypes, column names etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d60c8",
   "metadata": {},
   "source": [
    "The following columns are produced when we run `meta`:\n",
    "- c: column name\n",
    "- t: <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">column type</a>\n",
    "- f: <a href=\"https://code.kx.com/q4m3/8_Tables/#85-foreign-keys-and-virtual-columns\" target=\"_blank\">foreign keys</a>\n",
    "- a: <a href=\"https://code.kx.com/q/ref/set-attribute/\" target=\"_blank\">attributes</a> (modifiers applied for performance optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1a6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                          sym  price    size\n",
      "------------------------------------------------\n",
      "2025.01.02D00:00:00.000000000 GOOG 9.135562 163 \n",
      "2025.01.02D00:00:00.000000000 AMZN 50.55135 486 \n",
      "2025.01.02D00:00:00.000000000 GOOG 19.24164 294 \n",
      "2025.01.02D00:00:00.000000000 AMZN 61.35597 22  \n",
      "2025.01.02D00:00:00.000000000 GOOG 65.6921  939 \n",
      "2025.01.02D00:00:00.000000000 AAPL 50.87575 208 \n",
      "2025.01.02D00:00:00.000000000 GOOG 10.11053 961 \n",
      "2025.01.02D00:00:00.000000000 AAPL 2.799731 253 \n",
      "2025.01.02D00:00:00.000000000 AMZN 62.04305 74  \n",
      "2025.01.02D00:00:00.000000000 AAPL 41.01317 293 \n"
     ]
    }
   ],
   "source": [
    "10 sublist trade         / get first 10 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2a1d-2ca1-4235-a6d9-28779a361fdd",
   "metadata": {},
   "source": [
    "## 3.  Save Data to Disk\n",
    "\n",
    "Once the data is generated, youâ€™ll likely want to save it to disk for persistent storage.\n",
    "\n",
    "Because we want the ability to scale, partitioning by date will be a good approach for this dataset. Without partitioning, queries that span large time periods would require scanning entire datasets, which can be very slow and resource-intensive. By partitioning data, kdb+ can limit the query scope to the relevant partitions, significantly speeding up the process.\n",
    "\n",
    "To partition by date we can use the inbuilt function [`.Q.dpft`](#https://code.kx.com/q/ref/dotq/#dpft-save-table).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a070de1a-2796-47ca-935b-d2fd38d0ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n"
     ]
    }
   ],
   "source": [
    "homeDir:getenv[`HOME]                    / Get the home directory for edu.kx.com\n",
    "dbDir:homeDir,\"/data\"                    / Define database location as string\n",
    "dbPath:hsym `$dbDir                      / Database location as hsym for file I/O\n",
    ".Q.dpft[dbPath;day;`sym;`trade]          / Save data as a partitioned database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914853",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- <a href=\"https://code.kx.com/q/ref/hsym/\" target=\"_blank\">hsym</a>: This function prefixes the directory location with a colon to make it a file handle\n",
    "- <a href=\"https://code.kx.com/q/ref/dotq/#dpft-save-table\" target=\"_blank\">.Q.dpft[d;p;f;t]</a>: This command saves data to a <b>(d)</b>atabase location, targeting a particular <b>(p)</b>artition and indexes the data on a chosen <b>(f)</b>ield for the specified <b>(t)</b>able.\n",
    "\n",
    "One persisted, the table name is returned. We can test its worked as expected by deleting the `trade` table we have in memory and reloading the database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     / Delete in memory table\n",
    "system\"l \",dbDir                         / Load the partitioned database\n",
    "meta trade                               / Check it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bdda4",
   "metadata": {},
   "source": [
    "kdb+ actually offers a number of different methods to store tables which will allow for efficient storage and querying for different sized datasets: flat, splayed, partitioned and segmented.\n",
    "\n",
    "A general rule of thumb around which format to choose depends on three things:\n",
    "\n",
    "- Will the table continue to grow at a fast rate?\n",
    "- Am I working in a RAM/memory constrained environment?\n",
    "- What level of performance do I want?\n",
    "\n",
    "To learn more about these types and when to choose which <a href=\"https://code.kx.com/q/database/\" target=\"_blank\">see here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0265c3d",
   "metadata": {},
   "source": [
    "## 4. Scaling Data Ingestion with Functions\n",
    "\n",
    "If you want to scale the ingestion of data to many days, itâ€™s helpful to create a reusable function. Letâ€™s create a function `createTrade` that generates trade data for specific dates and saves it to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46ebc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`trade`trade`trade`trade`trade\n"
     ]
    }
   ],
   "source": [
    "createTrade:{[date]                                            / Start of function definition and input parameters\n",
    "    trade::([]                                                 / Start of table definition\n",
    "              time:asc (`timestamp$date) + n?24:00:00;         / Start from midnight, spread across 24h\n",
    "              sym:n?`AAPL`MSFT`GOOG`AMZN;                      / Random stock symbols\n",
    "              price:n?100f;                                    / Random trade prices\n",
    "              size:n?1000                                      / Random trade sizes\n",
    "        );                                                     / End of table definition\n",
    "    .Q.dpft[dbPath;date;`sym;`trade]                           / Save data as a partitioned database\n",
    " }                                                             / End of function definition\n",
    "\n",
    "days:2025.02.01 + til 5                                        / Generate a list of 5 dates\n",
    "createTrade each days                                          / Execute the function for each date in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba268e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- The function `createTrade` generates trade data for a given date, and then saves it to disk.\n",
    "- We generate data for multiple days (2025.02.01 to 2025.02.05), using the [`til`](#https://code.kx.com/q/ref/til/) operator as a quick handy way to generate a list of dates.\n",
    "- The we loop over the dates using [`each`](#https://code.kx.com/q/wp/iterators/#map-iterators)\n",
    "\n",
    "> **ðŸ“Œ Iterators** like each are the primary means of iteration in q, and in almost all cases the most efficient way to iterate. Loops are rare in q programs and are almost always candidates for optimization.\n",
    "\n",
    "After running this function, the data will be partitioned and stored for each specific day. Again, lets delete our in memory `trade` table and reload our database to pick up these new additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d876cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "date      | x      \n",
      "----------| -------\n",
      "2025.01.02| 5000000\n",
      "2025.02.01| 5000000\n",
      "2025.02.02| 5000000\n",
      "2025.02.03| 5000000\n",
      "2025.02.04| 5000000\n",
      "2025.02.05| 5000000\n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                               / Delete in memory table\n",
    "system\"l \",dbDir                                   / Load the partitioned database\n",
    "select count i by date from trade                  / Select number of records per date within the trade table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ac59",
   "metadata": {},
   "source": [
    "## 5. Time Series Analytics\n",
    "\n",
    "Now that we have some data, let's dive into some basic time-series analytics.\n",
    "\n",
    "### Total Trade Volume Every Hour for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| size    \n",
      "-----------------| --------\n",
      "2025.01.02 00:00 | 26125639\n",
      "2025.01.02 01:00 | 26087718\n",
      "2025.01.02 02:00 | 25850669\n",
      "2025.01.02 03:00 | 25931807\n",
      "2025.01.02 04:00 | 25985234\n",
      "2025.01.02 05:00 | 26030263\n",
      "2025.01.02 06:00 | 26054163\n",
      "2025.01.02 07:00 | 25967277\n",
      "2025.01.02 08:00 | 26090358\n",
      "2025.01.02 09:00 | 25781515\n",
      "2025.01.02 10:00 | 25881830\n",
      "2025.01.02 11:00 | 26232817\n",
      "2025.01.02 12:00 | 26134608\n",
      "2025.01.02 13:00 | 26233071\n",
      "2025.01.02 14:00 | 25750824\n",
      "2025.01.02 15:00 | 26258717\n",
      "2025.01.02 16:00 | 26115470\n",
      "2025.01.02 17:00 | 26280174\n",
      "2025.01.02 18:00 | 25846487\n",
      "2025.01.02 19:00 | 26101170\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select sum size \n",
    "    by date,\n",
    "       60 xbar time.minute \n",
    "    from trade \n",
    "    where sym=`AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944524",
   "metadata": {},
   "source": [
    "#### qSQL & Temporal Arithmetic\n",
    "Here we are using <a href=\"https://code.kx.com/q/basics/qsql/\" target=\"_blank\">qSQL</a>, the inbuilt table query language in kdb+. If you have used SQL, you will find the syntax of qSQL queries very similar.\n",
    "- Just as in SQL, table results called using `select` and `from` and can be filtered by expressions following a `where`\n",
    "- Multiple filter criteria, separated by ,, are evaluated starting from the left\n",
    "- To group similar values together we can use the `by` clause. This is particularly useful in combination with used with an aggregation like `sum`,`max`,`min` etc.\n",
    "\n",
    "kdb+/q supports several temporal types and arithmetic between them. See here for a summary of <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">datatypes</a>.\n",
    "\n",
    "In this example:\n",
    "- The `time` column in the data has a type of timestamp, which includes both date and time values.\n",
    "- We convert the `time` values to their minute values (including hours and minutes)\n",
    "- We then aggregate further on time by using <a href=\"https://code.kx.com/q/ref/xbar/\" target=\"_blank\">xbar</a> to bucket the minutes into hours (60-unit buckets)\n",
    "\n",
    "### Weighted Average Price and Last Trade Price Every 15 Minutes for MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1172a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| lastPx   vwapPx  \n",
      "-----------------| -----------------\n",
      "2025.01.02 00:00 | 34.53411 49.96859\n",
      "2025.01.02 00:15 | 90.29525 50.40277\n",
      "2025.01.02 00:30 | 19.18761 50.07307\n",
      "2025.01.02 00:45 | 29.66553 50.06219\n",
      "2025.01.02 01:00 | 16.29292 49.7994 \n",
      "2025.01.02 01:15 | 69.50636 49.72435\n",
      "2025.01.02 01:30 | 65.33913 49.67048\n",
      "2025.01.02 01:45 | 71.91969 49.5942 \n",
      "2025.01.02 02:00 | 76.23301 49.73794\n",
      "2025.01.02 02:15 | 91.7929  50.13313\n",
      "2025.01.02 02:30 | 46.957   49.84538\n",
      "2025.01.02 02:45 | 24.25048 50.22541\n",
      "2025.01.02 03:00 | 50.60707 49.95312\n",
      "2025.01.02 03:15 | 8.500885 50.11487\n",
      "2025.01.02 03:30 | 23.7883  49.59574\n",
      "2025.01.02 03:45 | 64.91134 49.62607\n",
      "2025.01.02 04:00 | 22.49924 49.86929\n",
      "2025.01.02 04:15 | 33.22247 49.57969\n",
      "2025.01.02 04:30 | 56.15438 50.40053\n",
      "2025.01.02 04:45 | 64.6472  49.79184\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    " by date, 15 xbar time.minute \n",
    " from trade \n",
    " where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc755377",
   "metadata": {},
   "source": [
    "This is similar to the previous analytic, but this time we make use of the built in `wavg` function to find out the weighted average over time intervals. \n",
    "\n",
    "In finance, volume-weighted averages give a more accurate reflection of a stockâ€™s price movement by incorporating trading volume at different price levels. This can be especially useful in understanding whether a price move is supported by strong market participation or is just a result of a few trades.\n",
    "\n",
    "Let's time this anayltic with `\\t` to see how long it takes in milliseconds to crunch through 30 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d040fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "\\t select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    "   by date, 15 xbar time.minute \n",
    "   from trade \n",
    "   where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea65524",
   "metadata": {},
   "source": [
    "The query processed 30+ million records in 147 ms, efficiently aggregating last price (`lastPx`) and volume-weighted-average price (`vwapPx`) for MSFT trades. The use of `by date, 15 xbar time.minute` optimized the grouping, making the computation fast. This demonstrates the power of kdb+/q for high-speed time-series analytics.\n",
    "\n",
    " ### SQL Comparison\n",
    "\n",
    "A SQL version of this query above would look something like:\n",
    "\n",
    "```\n",
    "\n",
    "SELECT \n",
    "    (array_agg(price ORDER BY time DESC))[1] AS lastPx,\n",
    "    SUM(price * size) / NULLIF(SUM(size), 0) AS vwapPx,\n",
    "    DATE_TRUNC('day', time),                                            \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE) \n",
    "FROM \n",
    "    trade\n",
    "WHERE \n",
    "    sym = 'MSFT'\n",
    "GROUP BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE)\n",
    "ORDER BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE);\n",
    "\n",
    "```\n",
    "\n",
    "SQL is more complex due to several factors:\n",
    "- **Time-series Calculations**: The SQL version involves the creation of custom logic for common time-series calculations such as volume-weighted-averages. In the q-sql version, these functionalities are implicit, and the syntax is more concise when working with vectors. The SQL equivalent requires custom definitions and is often more verbose leaving room for error.\n",
    "- **Grouping and Aggregation**: In the q-sql version, grouping by date and a 15 minute window is done with a single, simple syntax, which is an efficient and intuitive way to express time bucketing. In SQL, similar behavior requires explicitly defining how time intervals are handled and aggregating the results using GROUP BY with custom time expressions which are often repeated throughout the query.\n",
    "- **Temporal Formatting**: SQL queries often require repetitive conversion for handling timestamp formats, which is more cumbersome compared to q-sql, where time-based operations like xbar (interval-based bucketing) can be done directly in a streamlined manner. Temporal primitives also make it extremely easy to convert a nanosecond timestamp to it's equivalent minute using dot notation e.g. time.minute\n",
    "- **Data Transformation**: The q language is optimized for high-performance, in-memory, columnar data transformations, which allows for more compact expressions on vectors of data. SQL, on the other hand, is typically too general purpose for even simple transformations on time-series data. This is down to how kdb+/q is designed, where operations execute on ordered lists, whereas SQL (based on set theory) treats data as records instead of columns e.g. selecting the (last) value in a series, or understanding prior states (deltas) for series movements would require re-ordering the column data\n",
    "- **Performance Considerations**: q-sql is designed for high-performance analytics on large datasets, and many operations that would require complex SQL expressions can be done efficiently with q-sql syntax. In SQL, complex operations requires workarounds such as additional processing with temporary tables, sub-expressions, re-indexing, changing data models, or heavily leveraging partitions and window functions.\n",
    "\n",
    "Thus, while the core logic of the query is similar in both languages, the SQL version requires much more overhead in terms of complexity and verbosity. This inefficiency will also become more pronounced with large datasets, leading to challenges with query performance.\n",
    "\n",
    "While these are just basic analytics, they highlight kdb+/qâ€™s ability to storage and analyse large-scale time-series datasets quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b806e",
   "metadata": {},
   "source": [
    "## 6. Asof Join â€“ Matching Trades with Quotes\n",
    "\n",
    "One of the most powerful features in kdb+/q is the asof join (`aj`), which is designed to match records from two tables based on the most recent timestamp. Unlike a standard SQL join, where records must match exactly on a key, an asof join finds the most recent match.\n",
    "\n",
    "Why Use Asof Joins?\n",
    "In time-series data, we often deal with information arriving at different intervals. For example:\n",
    "- Trade and Quote Data: A trade occurs at a given time, and we want to match it with the latest available quote.\n",
    "- Sensor Data: A sensor records temperature every second, while another logs environmental data every 10 secondsâ€”matching the closest reading is crucial.\n",
    "\n",
    "> **ðŸ“Œ** kdb+/q optimizes asof joins to handle large datasets efficiently, making it a key tool in real-time analytics and historical data analysis.\n",
    "\n",
    "#### Generate synthetic quote data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44e0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:2000000\n",
    "today:last days\n",
    "quote:([] \n",
    "    time:asc (`timestamp$today) + n?86400000000000;  / Random timestamps\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                      / Random stock tickers\n",
    "    bid:n?100f;                                      / Random bid prices\n",
    "    ask:n?100f                                       / Random ask prices\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0b9f",
   "metadata": {},
   "source": [
    "As we're keeping this table in memory we need to perform one extra step before joining, we apply the parted (p#) attribute to the sym column of the quote table. Our trade table on disk already has the parted attribute on the sym column, we see this in the column `a` when we run `meta trade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4c9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781da43",
   "metadata": {},
   "source": [
    "This is crucial for optimizing asof joins, as it ensures faster lookups when performing symbol-based joins. Before applying parted to quote, we first sort the table by sym using [`xasc`](#https://code.kx.com/q/ref/asc/), as the parted attribute requires the column to be sorted for it to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote:`sym xasc quote                  / Sorting sym in ascending order\n",
    "quote:update `p#sym from quote         / Apply the parted attruibute on sym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fd87b",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `xasc` Sorts the quote table by sym in ascending order\n",
    "- `#`  Applies the parted attribute to sym, optimizing symbol-based lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a1a4",
   "metadata": {},
   "source": [
    "#### Peform Asof Join\n",
    "\n",
    "We now match each trade with the most recent available quote for todays date using [`aj`](#https://code.kx.com/q/ref/aj/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bb0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       sym  time                          price    size bid      ask      \n",
      "------------------------------------------------------------------------------\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 80.04666 14                     \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 53.82016 569                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 81.52125 521                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 31.45482 100                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 54.01513 610                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 73.68096 953                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 36.05031 909                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 73.13901 972                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.000000000 91.57547 102                    \n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 77.43928 365  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 67.8124  519  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 98.40835 873  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 61.49927 96   63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 20.99723 248  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 57.49622 45   63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 81.48007 37   63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 39.73889 890  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 95.2051  53   63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 89.32233 421  63.01776 0.9759341\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.000000000 84.94262 31   63.01776 0.9759341\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "tradequote:aj[`sym`time; select from trade where date=today; quote]\n",
    "tradequote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd376e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `aj` performs an asof join on the `sym` and `time` columns\n",
    "- Each trade record gets matched with the latest available quote at or before the tradeâ€™s timestamp.\n",
    "- We can see this means the first few `bid` and `ask` values are empty because there was no quote data prior to those trades.\n",
    "\n",
    "This approach ensures that for every trade, we have the best available quote information, allowing traders to analyze trade execution relative to the prevailing bid/ask spread at the time.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try [Example2](Example2.html) on Real-Time Ingestion & Streaming Analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
