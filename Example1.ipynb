{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3a307-a161-4d03-81b6-12388f7cbe3d",
   "metadata": {},
   "source": [
    "# Time Series & Historical Query Analysis\n",
    "\n",
    "Welcome to this example where we'll demonstrate how to work with large datasets in kdb+ to analyze time-series data. \n",
    "\n",
    "One of the key features of kdb+ is its ability to handle huge volumes of data with exceptional speed and efficiency. Whether it's reading massive datasets, performing time-based aggregations, or joining data from different sources, kdb+ excels at time-series analysis. By the end of this example, you'll have a clear understanding of how to create, manipulate, store, and analyze data using q/kdb+. Along the way, we'll introduce several key concepts that are fundamental to working with q/kdb+.\n",
    "\n",
    "\n",
    "Here, we'll cover:\n",
    "- Creating a large time-series dataset from scratch\n",
    "- Saving this data to a database on disk\n",
    "- Streamline ingestion and save down using functions \n",
    "- Performing time-based aggregations to analyze trends over time\n",
    "- Using asof joins (aj) to combine time-series data (e.g., matching trades to quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c6c31-ab9a-4349-b0f9-5c541553f284",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "1. For setup instructions and prerequisites, please refer to the [README](README.md).\n",
    "2. Ensure PyKX is properly initialized and qfirst mode is enabled by running the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKX now running in 'jupyter_qfirst' mode. All cells by default will be run as q code. \n",
      "Include '%%python' at the beginning of each cell to run as python code. \n"
     ]
    }
   ],
   "source": [
    "import pykx as kx\n",
    "kx.util.jupyter_qfirst_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203051bc-25ce-441f-ac35-e078d2499a38",
   "metadata": {},
   "source": [
    "## 2. Create the Time Series Dataset\n",
    "\n",
    "Let’s start by creating a sample dataset to work with. This dataset will simulate trade data over a period of time, with random values for price, size, and symbols. We’ll generate 5 million rows of trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b63cdf-8139-4b53-810f-3084fdb1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:5000000\n",
    "day:2025.01.02\n",
    "trade:([] \n",
    "    time:asc (`timestamp$day) + n?24:00:00.000000000;    / Start from midnight, spread across 24h\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                          / Random symbols\n",
    "    price:n?100f;                                        / Random prices\n",
    "    size:n?1000                                          / Random trade sizes\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691b30-afb5-4f3b-b9d5-680537770eb9",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening:\n",
    "- `n: 5000000` sets the number of rows we want to generate\n",
    "- We define a new table with table notation `([] col1:<values>; col2:<values>: ....)`\n",
    "- We use `?` to generate random values for 4 columns:\n",
    "    - `time` is populated with timestamps starting from midnight and increasing across a 24-hour period, with a random offset to simulate a spread of trades.\n",
    "    - `sym` is populated with random symbols like AAPL, MSFT, etc., selected from a list.\n",
    "    - `price` and trade `size` are randomnly generated\n",
    "\n",
    "This table is now available in memory to investigate and query. Let's take a quick look at the row [`count`](#https://code.kx.com/q/ref/count/), schema details with [`meta`](#https://code.kx.com/q/ref/meta/) and first 10 rows using [`sublist`](#https://code.kx.com/q/ref/sublist/).\n",
    "\n",
    "These simple commands are essential when exploring your data quickly in q/kdb+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfb0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n"
     ]
    }
   ],
   "source": [
    "count trade       / get row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748c29a6-ff5d-477f-ac9b-18ff47fc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "time | p   s\n",
      "sym  | s    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta trade        / get table schema details - datatypes, column names etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d60c8",
   "metadata": {},
   "source": [
    "The following columns are produced when we run `meta`:\n",
    "- c: column name\n",
    "- t: [column type](#https://code.kx.com/q/ref/#datatypes)\n",
    "- f: [foreign keys](#https://code.kx.com/q4m3/8_Tables/#85-foreign-keys-and-virtual-columns)\n",
    "- a: [attributes](#https://code.kx.com/q/ref/#attributes): modifiers applied for performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1a6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                          sym  price     size\n",
      "-------------------------------------------------\n",
      "2025.01.02D00:00:00.011406093 AAPL 5.286875  908 \n",
      "2025.01.02D00:00:00.014765560 AMZN 47.93312  360 \n",
      "2025.01.02D00:00:00.038664042 GOOG 17.13715  522 \n",
      "2025.01.02D00:00:00.046268105 AMZN 70.1903   257 \n",
      "2025.01.02D00:00:00.050713866 AMZN 23.25251  858 \n",
      "2025.01.02D00:00:00.073526054 AMZN 18.48452  585 \n",
      "2025.01.02D00:00:00.099858641 AAPL 66.48997  90  \n",
      "2025.01.02D00:00:00.120478123 AAPL 29.23461  683 \n",
      "2025.01.02D00:00:00.156366080 GOOG 0.8593363 90  \n",
      "2025.01.02D00:00:00.165257602 GOOG 75.44551  869 \n"
     ]
    }
   ],
   "source": [
    "10 sublist trade  / get first 10 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2a1d-2ca1-4235-a6d9-28779a361fdd",
   "metadata": {},
   "source": [
    "## 3.  Save Data to Disk\n",
    "\n",
    "Once the data is generated, you’ll likely want to save it to disk for persistent storage.\n",
    "\n",
    "Because we want the ability to scale, partitioning by date will be a good approach for this dataset. Without partitioning, queries that span large time periods would require scanning entire datasets, which can be very slow and resource-intensive. By partitioning data, kdb+ can limit the query scope to the relevant partitions, significantly speeding up the process.\n",
    "\n",
    "To partition by date we can use the inbuilt function [`.Q.dpft`](#https://code.kx.com/q/ref/dotq/#dpft-save-table).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a070de1a-2796-47ca-935b-d2fd38d0ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n"
     ]
    }
   ],
   "source": [
    "dbDir:\"/home/your-dir/data\"          / Define database location\n",
    "dbPath:hsym `$dbDir\n",
    ".Q.dpft[dbPath;day;`sym;`trade]            / Save data as a partitioned database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914853",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- [`hsym`](#https://code.kx.com/q/ref/hsym/): This function prefixes the directory location with a colon to make it a file handle\n",
    "- `.Q.dpft[d;p;f;t]`: This command persists to a (d)atabase location with a specific (p)artition with data from a (t)able with an associated (f)ield.\n",
    "\n",
    "One persisted, the table name is returned. We can test its worked as expected by deleting the `trade` table we have in memory and reloading the database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     / Delete in memory table\n",
    "system\"l \",dbDir                         / Load the partitioned database\n",
    "meta trade                               / Check it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bdda4",
   "metadata": {},
   "source": [
    "kdb+ actually offers a number of different methods to store tables which will allow for efficient storage and querying for different sized datasets: flat, splayed, partitioned and segmented.\n",
    "\n",
    "A general rule of thumb around which format to choose depends on three things:\n",
    "\n",
    "- Will the table continue to grow at a fast rate?\n",
    "- Am I working in a RAM constrained environment?\n",
    "- What level of performance do I want?\n",
    "\n",
    "To learn more about these types and when to choose which [see here](#https://code.kx.com/q/database/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0265c3d",
   "metadata": {},
   "source": [
    "## 4. Scaling Data Ingestion with Functions\n",
    "\n",
    "If you want to scale the ingestion of data to many days, it’s helpful to create a reusable function. Let’s create a function `createTrade` that generates trade data for specific dates and saves it to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46ebc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`trade`trade`trade`trade`trade\n"
     ]
    }
   ],
   "source": [
    "createTrade:{[date]\n",
    "    trade::([] time:asc (`timestamp$date) + n?24:00:00.000000000; / Start from midnight, spread across 24h\n",
    "              sym:n?`AAPL`MSFT`GOOG`AMZN;                         / Random symbols\n",
    "              price:n?100f;                                       / Random prices\n",
    "              size:n?1000);                                       / Random trade sizes\n",
    "    .Q.dpft[dbPath;date;`sym;`trade]                              / Save data as a partitioned database\n",
    " }\n",
    "\n",
    "days:2025.02.01 + til 5\n",
    "createTrade each days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba268e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- The function `createTrade` generates trade data for a given date, and then saves it to disk.\n",
    "- We generate data for multiple days (2025.02.01 to 2025.02.05), using the [`til`](#https://code.kx.com/q/ref/til/) operator as a quick handy way to generate a list of dates.\n",
    "- The we loop over the dates using [`each`](#https://code.kx.com/q/wp/iterators/#map-iterators)\n",
    "\n",
    "> **📌 Iterators** like each are the primary means of iteration in q, and in almost all cases the most efficient way to iterate. Loops are rare in q programs and are almost always candidates for optimization.\n",
    "\n",
    "After running this function, the data will be partitioned and stored for each specific day. Again, lets delete our in memory `trade` table and reload our database to pick up these new additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d876cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "date      | x      \n",
      "----------| -------\n",
      "2025.01.02| 5000000\n",
      "2025.02.01| 5000000\n",
      "2025.02.02| 5000000\n",
      "2025.02.03| 5000000\n",
      "2025.02.04| 5000000\n",
      "2025.02.05| 5000000\n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     / Delete in memory table\n",
    "system\"l \",dbDir                         / Load the partitioned database\n",
    "select count i by date from trade        / Count num rows by date after partitioning 5 days of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ac59",
   "metadata": {},
   "source": [
    "## 5. Time Series Analytics\n",
    "\n",
    "Now that we have some data, let's dive into some basic time-series analytics.\n",
    "\n",
    "### Total Trade Volume Every Hour for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| size    \n",
      "-----------------| --------\n",
      "2025.01.02 00:00 | 25891446\n",
      "2025.01.02 01:00 | 26050097\n",
      "2025.01.02 02:00 | 26147341\n",
      "2025.01.02 03:00 | 26037462\n",
      "2025.01.02 04:00 | 26043744\n",
      "2025.01.02 05:00 | 26024765\n",
      "2025.01.02 06:00 | 25993388\n",
      "2025.01.02 07:00 | 25923493\n",
      "2025.01.02 08:00 | 25926785\n",
      "2025.01.02 09:00 | 25889219\n",
      "2025.01.02 10:00 | 26145203\n",
      "2025.01.02 11:00 | 25802003\n",
      "2025.01.02 12:00 | 26088725\n",
      "2025.01.02 13:00 | 26144928\n",
      "2025.01.02 14:00 | 26120175\n",
      "2025.01.02 15:00 | 26083944\n",
      "2025.01.02 16:00 | 26226585\n",
      "2025.01.02 17:00 | 25882417\n",
      "2025.01.02 18:00 | 26146353\n",
      "2025.01.02 19:00 | 25890825\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select sum size \n",
    "    by date,\n",
    "       60 xbar time.minute \n",
    "    from trade \n",
    "    where sym=`AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944524",
   "metadata": {},
   "source": [
    "#### qSQL & Temporal Arithmetic\n",
    "Here we are using [qSQL](#https://code.kx.com/q/basics/qsql/), the inbuilt table query language in kdb+. If you have used SQL, you will find the syntax of qSQL queries very similar.\n",
    "- Just as in SQL, table results called using `select` and `from` and can be filtered by expressions following a `where`\n",
    "- Multiple filter criteria, separated by ,, are evaluated starting from the left\n",
    "- To group similar values together we can use the `by` clause. This is particularly useful in combination with used with an aggregation like `sum`,`max`,`min` etc.\n",
    "\n",
    "q/kdb+ supports several temporal types and arithmetic between them. See here for a summary of [datatypes](#https://code.kx.com/q/ref/#datatypes).\n",
    "In this example:\n",
    "- The `time` column in the data has a type of timestamp, which includes both date and time values.\n",
    "- We convert the `time` values to their minute values (including hours and minutes)\n",
    "- We then aggregate further on time by using [`xbar`](#https://code.kx.com/q/ref/xbar/) to bucket the minutes into hours (60-unit buckets)\n",
    "\n",
    "### Weighted Average Price and Last Trade Price Every 15 Minutes for MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1172a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| LastPrice WeightedPrice\n",
      "-----------------| -----------------------\n",
      "2025.01.02 00:00 | 11.43895  49.69477     \n",
      "2025.01.02 00:15 | 42.51047  49.91692     \n",
      "2025.01.02 00:30 | 69.80891  50.01703     \n",
      "2025.01.02 00:45 | 63.15883  49.95182     \n",
      "2025.01.02 01:00 | 5.080531  50.30521     \n",
      "2025.01.02 01:15 | 73.5871   49.87864     \n",
      "2025.01.02 01:30 | 89.19987  50.40955     \n",
      "2025.01.02 01:45 | 47.07693  49.92108     \n",
      "2025.01.02 02:00 | 13.39698  49.89728     \n",
      "2025.01.02 02:15 | 16.23821  50.17888     \n",
      "2025.01.02 02:30 | 87.14231  49.3321      \n",
      "2025.01.02 02:45 | 75.71376  50.28474     \n",
      "2025.01.02 03:00 | 12.01796  50.28201     \n",
      "2025.01.02 03:15 | 87.38662  50.45316     \n",
      "2025.01.02 03:30 | 28.95476  50.30487     \n",
      "2025.01.02 03:45 | 33.23928  50.07296     \n",
      "2025.01.02 04:00 | 63.04001  50.16875     \n",
      "2025.01.02 04:15 | 37.40805  50.05399     \n",
      "2025.01.02 04:30 | 78.51536  50.04083     \n",
      "2025.01.02 04:45 | 24.88957  50.66176     \n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select LastPrice:last price, \n",
    "       WeightedPrice:size wavg price\n",
    " by date,15 xbar time.minute \n",
    " from trade \n",
    " where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc755377",
   "metadata": {},
   "source": [
    "This is similar to the previous analytic, but this time we make use of the built in `wavg` function to find out the weighted average over time intervals. \n",
    "\n",
    "In finance, volume-weighted averages give a more accurate reflection of a stock’s price movement by incorporating trading volume at different price levels. This can be especially useful in understanding whether a price move is supported by strong market participation or is just a result of a few trades.\n",
    "\n",
    "Let's time this anayltic with `\\t` to see how long it takes in milliseconds to crunch through 30 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d040fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "\\t select LastPrice:last price, \n",
    "       WeightedPrice:size wavg price\n",
    " by date,15 xbar time.minute \n",
    " from trade \n",
    " where sym=`MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea65524",
   "metadata": {},
   "source": [
    "The query processed 30+ million records in 147 ms, efficiently aggregating LastPrice and WeightedPrice for MSFT trades. The use of `by date, 15 xbar time.minute` optimized the grouping, making the computation fast. This demonstrates the power of kdb+/q for high-speed time-series analytics.\n",
    "\n",
    " ### SQL Comparison\n",
    "\n",
    "A SQL version of this query above would look something like:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE function wavg(v,p) AS sum(v * p)/sum(v);\n",
    "CREATE OR REPLACE MACRO xbarTime(n,x) AS n*(((date('hour', x::Time)*60) + date('minute', x::Time))//n);\n",
    "select sym, wavg(\"size\", \"price\") AS wavg FROM trade GROUP BY ALL ORDER BY sym;\n",
    "select min(\"time\") AS time, last(\"price\") as LastPrice,wavg(\"size\", \"price\") AS WeightedPrice \n",
    "    FROM trade \n",
    "    WHERE sym='MSFT' GROUP BY xbarTime(15,time) ORDER BY time;\n",
    "```\n",
    "\n",
    "SQL is more complex to write due to several factors:\n",
    "- **Custom Functions**: The SQL version involves the creation of custom functions and macros such as wavg(v, p) for weighted averages and xbarTime(n, x) for time bucketing. In the Q version, these functionalities are implicit, and the syntax is more concise. The SQL equivalent requires explicit definitions and can be more verbose.\n",
    "- **Grouping and Aggregatio**n**: In the q version, grouping by date and 15 xbar time.minute is done with a single, simple syntax, which is efficient and easy to express. In SQL, similar behavior requires explicitly defining how time intervals are handled and aggregating the results using GROUP BY and custom time expressions.\n",
    "- **Time Formatting**: SQL queries often require conversion or handling of time formats, which is more cumbersome compared to q, where time-based operations like xbar (interval-based bucketing) can be done directly in a more streamlined manner.\n",
    "- **Data Transformation**: The q language is optimized for high-performance, in-memory, columnar data transformations, which allows for more compact expressions. SQL, on the other hand, typically requires a more rigid structure for achieving the same results, often relying on the use of subqueries or joining intermediate results.\n",
    "- **Performance Considerations**: q is designed for high-performance analytics on large datasets, and many operations that would require more complex SQL expressions can be done more efficiently with Q syntax. In SQL, complex operations may require additional processing, such as temporary tables, indexing, or window functions.\n",
    "\n",
    "Thus, while the core logic of the query is similar in both languages, the SQL version requires more manual setup (e.g., custom function creation, complex time transformations, and explicit grouping), leading to a more verbose and complex query.\n",
    "\n",
    "While these are just basic analytics, but they showcase q/kdb+’s ability to handle large-scale time-series data and perform aggregations quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b806e",
   "metadata": {},
   "source": [
    "## 6. Asof Join – Matching Trades with Quotes\n",
    "\n",
    "One of the most powerful features in q/kdb+ is the asof join (`aj`), which is designed to match records from two tables based on the most recent timestamp. Unlike a standard SQL join, where records must match exactly on a key, an asof join finds the most recent match.\n",
    "\n",
    "Why Use Asof Joins?\n",
    "In time-series data, we often deal with information arriving at different intervals. For example:\n",
    "- Trade and Quote Data: A trade occurs at a given time, and we want to match it with the latest available quote.\n",
    "- Sensor Data: A sensor records temperature every second, while another logs environmental data every 10 seconds—matching the closest reading is crucial.\n",
    "\n",
    "> **📌** q/kdb+ optimizes asof joins to handle large datasets efficiently, making it a key tool in real-time analytics and historical data analysis.\n",
    "\n",
    "#### Generate synthetic quote data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:2000000\n",
    "today:last days\n",
    "quote:([] \n",
    "    time:asc (`timestamp$today) + n?86400000000000;  / Random timestamps\n",
    "    sym:n?`AAPL`MSFT`GOOG`AMZN;                     / Symbols\n",
    "    bid:n?100f;                                     / Random bid prices\n",
    "    ask:n?100f                                      / Random ask prices\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0b9f",
   "metadata": {},
   "source": [
    "As we're keeping this table in memory we need to perform one extra step before joining, we apply the parted (p#) attribute to the sym column of the quote table. Our trade table on disk already has the parted attribute on the sym column, we see this in the column `a` when we run `meta trade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4c9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781da43",
   "metadata": {},
   "source": [
    "This is crucial for optimizing asof joins, as it ensures faster lookups when performing symbol-based joins. Before applying parted to quote, we first sort the table by sym using [`xasc`](#https://code.kx.com/q/ref/asc/), as the parted attribute requires the column to be sorted for it to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote:`sym xasc quote           / sorting sym in ascending order\n",
    "quote:update `p#sym from quote  / apply parted attruibute on sym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fd87b",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `xasc` Sorts the quote table by sym in ascending order\n",
    "- `#`  Applies the parted attribute to sym, optimizing symbol-based lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a1a4",
   "metadata": {},
   "source": [
    "#### Peform Asof Join\n",
    "\n",
    "We now match each trade with the most recent available quote for todays date using [`aj`](#https://code.kx.com/q/ref/aj/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0bb0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       sym  time                          price    size bid      ask     \n",
      "-----------------------------------------------------------------------------\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.041379779 68.3932  935                   \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.062924623 60.90381 405                   \n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.173867493 16.86426 495  40.66565 73.38496\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.233070552 50.05196 816  40.66565 73.38496\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.338360667 30.29596 67   40.66565 73.38496\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.349666178 76.59111 689  40.66565 73.38496\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.431198626 20.13091 740  94.77276 3.518068\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.515386462 33.70869 306  94.77276 3.518068\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.765958428 22.29313 60   51.94371 83.79661\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.777746737 29.0045  976  51.94371 83.79661\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.904098898 57.4872  201  59.59065 61.72714\n",
      "2025.02.05 AAPL 2025.02.05D00:00:00.922767072 80.19457 23   59.59065 61.72714\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.012547314 10.01102 568  10.23587 48.05871\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.075029373 97.22168 748  10.23587 48.05871\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.075391471 5.462816 755  10.23587 48.05871\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.158231496 70.71495 16   10.23587 48.05871\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.200757920 67.03303 290  15.92678 66.94334\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.300958544 15.88646 118  9.056417 69.61164\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.331093162 34.96063 319  9.056417 69.61164\n",
      "2025.02.05 AAPL 2025.02.05D00:00:01.343666017 45.42165 480  9.056417 69.61164\n",
      "..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:20:47:102 [kurl] INFO - Replacing oauth2 token\n",
      "15:20:47:102 [kurl] INFO - Replacing oauth2 token\n"
     ]
    }
   ],
   "source": [
    "tradequote:aj[`sym`time; \n",
    "              select from trade where date=today;\n",
    "              quote]\n",
    "tradequote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd376e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `aj` performs an asof join on the `sym` and `time` columns\n",
    "- Each trade record gets matched with the latest available quote at or before the trade’s timestamp.\n",
    "- We can see this means the first few `bid` and `ask` values are empty because there was no quote data prior to those trades.\n",
    "\n",
    "This approach ensures that for every trade, we have the best available quote information, allowing traders to analyze trade execution relative to the prevailing bid/ask spread at the time.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try [Example2](Example2.html) on Real-Time Ingestion & Streaming Analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
